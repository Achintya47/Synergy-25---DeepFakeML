{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13670740,"sourceType":"datasetVersion","datasetId":8692411}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport timm\nimport time\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport psutil\nimport os\nimport json\n\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        return F.relu(out)\n\n\nclass SEBlock(nn.Module):\n    def __init__(self, channels, reduction=16):\n        super().__init__()\n        self.squeeze = nn.AdaptiveAvgPool2d(1)\n        self.excitation = nn.Sequential(\n            nn.Linear(channels, channels // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channels // reduction, channels),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.squeeze(x).view(b, c)\n        y = self.excitation(y).view(b, c, 1, 1)\n        return x * y\n\n\nclass ImprovedCustomCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n\n        self.layer1 = self._make_layer(64, 128, 2, stride=2)\n        self.layer2 = self._make_layer(128, 256, 2, stride=2)\n        self.layer3 = self._make_layer(256, 512, 2, stride=2)\n\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Dropout(0.4),\n            nn.Linear(512, 256),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2)\n        )\n\n    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n        layers = [ResidualBlock(in_channels, out_channels, stride)]\n        for _ in range(1, num_blocks):\n            layers.append(ResidualBlock(out_channels, out_channels))\n        layers.append(SEBlock(out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n\ndef get_model_size(model):\n    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n    return (param_size + buffer_size) / 1024**2\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef get_gpu_memory():\n    return torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0\n\ndef get_cpu_memory():\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / 1024**2\n\n\n\ndef quantize_model_dynamic(model):\n    \"\"\"Dynamic quantization (CPU only).\"\"\"\n    return torch.quantization.quantize_dynamic(\n        model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8\n    )\n\ndef load_models(device):\n    print(\"\\n\" + \"=\"*70)\n    print(\" LOADING MODELS & WEIGHTS \")\n    print(\"=\"*70)\n\n    models = {\n        \"ConvNeXt-Tiny\": timm.create_model(\"convnext_tiny\", pretrained=False, num_classes=2),\n        \"EfficientNet-B0\": timm.create_model(\"efficientnet_b0\", pretrained=False, num_classes=2),\n        \"Custom-ResNet-SE\": ImprovedCustomCNN()\n    }\n\n    weight_paths = {\n        \"ConvNeXt-Tiny\": Path(\"/kaggle/input/models/convnext_final.pth\"),\n        \"EfficientNet-B0\": Path(\"/kaggle/input/models/efficientnet_final.pth\"),\n        \"Custom-ResNet-SE\": Path(\"/kaggle/input/models/custom_cnn_final.pth\")\n    }\n\n    for name, model in list(models.items()):\n        path = weight_paths[name]\n        if path.exists():\n            print(f\" Loaded weights for {name} from {path.name}\")\n            model.load_state_dict(torch.load(path, map_location=\"cpu\"))\n        else:\n            print(f\" Weights not found for {name} ({path.name}), skipping this model.\")\n            models.pop(name)\n            continue\n        model.to(device).eval()\n\n    return models\n\n\ndef warmup_model(model, input_shape, device, num_iterations=10):\n    dummy_input = torch.randn(input_shape).to(device)\n    model.eval()\n    with torch.no_grad():\n        for _ in range(num_iterations):\n            _ = model(dummy_input)\n    if device.type == \"cuda\":\n        torch.cuda.synchronize()\n\ndef benchmark_inference(model, batch_size, num_iterations, device, model_name):\n    print(f\"\\n{'='*70}\\nBenchmarking: {model_name}\\n{'='*70}\")\n    input_shape = (batch_size, 3, 224, 224)  # standard image size\n    warmup_model(model, input_shape, device)\n    dummy_input = torch.randn(input_shape).to(device)\n\n    times = []\n    mem_before_cpu = get_cpu_memory()\n    mem_before_gpu = get_gpu_memory() if device.type == \"cuda\" else 0\n\n    model.eval()\n    with torch.no_grad():\n        for _ in tqdm(range(num_iterations), desc=model_name):\n            if device.type == \"cuda\":\n                torch.cuda.synchronize()\n            start = time.perf_counter()\n            _ = model(dummy_input)\n            if device.type == \"cuda\":\n                torch.cuda.synchronize()\n            times.append((time.perf_counter() - start) * 1000)\n\n    mem_after_cpu = get_cpu_memory()\n    mem_after_gpu = get_gpu_memory() if device.type == \"cuda\" else 0\n\n    times = np.array(times)\n    stats = {\n        \"model_name\": model_name,\n        \"mean_time_ms\": np.mean(times),\n        \"std_time_ms\": np.std(times),\n        \"throughput_imgs_sec\": (batch_size * 1000) / np.mean(times),\n        \"model_size_mb\": get_model_size(model),\n        \"parameters\": count_parameters(model),\n        \"cpu_memory_mb\": mem_after_cpu - mem_before_cpu,\n        \"gpu_memory_mb\": mem_after_gpu - mem_before_gpu,\n        \"times\": times.tolist()\n    }\n\n    print(f\"\\nMean Time: {stats['mean_time_ms']:.2f} ms | \"\n          f\"Throughput: {stats['throughput_imgs_sec']:.2f} img/s | \"\n          f\"Size: {stats['model_size_mb']:.2f} MB\")\n\n    return stats\n\n\ndef print_summary_table(results_fp32, results_quantized):\n    print(\"\\n\" + \"=\"*100)\n    print(\"PERFORMANCE SUMMARY TABLE\")\n    print(\"=\"*100)\n    print(f\"\\n{'Model':<20} | {'Precision':<10} | {'Time (ms)':<12} | {'Throughput':<15} | {'Size (MB)':<12}\")\n    print(\"-\" * 100)\n\n    models = list(set([r[\"model_name\"].split(\" (\")[0] for r in results_fp32]))\n    for model in models:\n        fp32 = next(r for r in results_fp32 if r[\"model_name\"].split(\" (\")[0] == model)\n        quant = next((r for r in results_quantized if r[\"model_name\"].split(\" (\")[0] == model), None)\n        if not quant:\n            continue\n\n        speedup = fp32[\"mean_time_ms\"] / quant[\"mean_time_ms\"]\n        reduction = (1 - quant[\"model_size_mb\"] / fp32[\"model_size_mb\"]) * 100\n\n        print(f\"{model:<20} | FP32       | {fp32['mean_time_ms']:>10.2f} | {fp32['throughput_imgs_sec']:>13.2f} | {fp32['model_size_mb']:>10.2f}\")\n        print(f\"{'':<20} | Quantized  | {quant['mean_time_ms']:>10.2f} | {quant['throughput_imgs_sec']:>13.2f} | {quant['model_size_mb']:>10.2f}\")\n        print(f\"{'':<20} | Improvement| {speedup:>9.2f}x | {((quant['throughput_imgs_sec']/fp32['throughput_imgs_sec'] - 1)*100):>11.1f}% | {reduction:>9.1f}%↓\")\n        print(\"-\" * 100)\n\n\n\ndef main():\n    print(\"\\n\" + \"=\"*70)\n    print(\" DEEPFAKE DETECTION MODEL STRESS TEST \")\n    print(\"=\"*70)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    batch_size, num_iterations = 64, 100\n    models = load_models(device)\n\n    # FP32 benchmark\n    results_fp32 = [benchmark_inference(m, batch_size, num_iterations, device, n)\n                    for n, m in models.items()]\n\n    # Quantization benchmark (CPU only)\n    print(\"\\n\" + \"=\"*70)\n    print(\" QUANTIZATION PHASE \")\n    print(\"=\"*70)\n\n    quantized_models = {n: quantize_model_dynamic(m.cpu()) for n, m in models.items()}\n    results_quantized = [benchmark_inference(m, batch_size, num_iterations, torch.device(\"cpu\"), f\"{n} (Quantized)\")\n                         for n, m in quantized_models.items()]\n\n    print_summary_table(results_fp32, results_quantized)\n\n    results = {\n        \"fp32\": results_fp32,\n        \"quantized\": results_quantized,\n        \"config\": {\n            \"batch_size\": batch_size,\n            \"iterations\": num_iterations,\n            \"device\": str(device)\n        }\n    }\n\n    with open(\"stress_test_results.json\", \"w\") as f:\n        json.dump(results, f, indent=4)\n    print(\"\\nResults saved to stress_test_results.json\\n\")\n\n\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T18:52:50.268466Z","iopub.execute_input":"2025-11-09T18:52:50.269079Z","iopub.status.idle":"2025-11-09T19:48:16.260918Z","shell.execute_reply.started":"2025-11-09T18:52:50.269045Z","shell.execute_reply":"2025-11-09T19:48:16.260123Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n DEEPFAKE DETECTION MODEL STRESS TEST \n======================================================================\nUsing device: cuda\n\n======================================================================\n LOADING MODELS & WEIGHTS \n======================================================================\n Loaded weights for ConvNeXt-Tiny from convnext_final.pth\n Loaded weights for EfficientNet-B0 from efficientnet_final.pth\n Loaded weights for Custom-ResNet-SE from custom_cnn_final.pth\n\n======================================================================\nBenchmarking: ConvNeXt-Tiny\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"ConvNeXt-Tiny: 100%|██████████| 100/100 [00:15<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nMean Time: 150.73 ms | Throughput: 424.61 img/s | Size: 106.13 MB\n\n======================================================================\nBenchmarking: EfficientNet-B0\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0: 100%|██████████| 100/100 [00:04<00:00, 22.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nMean Time: 43.29 ms | Throughput: 1478.26 img/s | Size: 15.46 MB\n\n======================================================================\nBenchmarking: Custom-ResNet-SE\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Custom-ResNet-SE: 100%|██████████| 100/100 [00:19<00:00,  5.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nMean Time: 196.96 ms | Throughput: 324.94 img/s | Size: 42.75 MB\n\n======================================================================\n QUANTIZATION PHASE \n======================================================================\n\n======================================================================\nBenchmarking: ConvNeXt-Tiny (Quantized)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"ConvNeXt-Tiny (Quantized): 100%|██████████| 100/100 [09:42<00:00,  5.82s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nMean Time: 5823.50 ms | Throughput: 10.99 img/s | Size: 7.28 MB\n\n======================================================================\nBenchmarking: EfficientNet-B0 (Quantized)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"EfficientNet-B0 (Quantized): 100%|██████████| 100/100 [06:01<00:00,  3.62s/it]\n","output_type":"stream"},{"name":"stdout","text":"\nMean Time: 3618.58 ms | Throughput: 17.69 img/s | Size: 15.45 MB\n\n======================================================================\nBenchmarking: Custom-ResNet-SE (Quantized)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Custom-ResNet-SE (Quantized): 100%|██████████| 100/100 [33:41<00:00, 20.22s/it]","output_type":"stream"},{"name":"stdout","text":"\nMean Time: 20214.20 ms | Throughput: 3.17 img/s | Size: 42.08 MB\n\n====================================================================================================\nPERFORMANCE SUMMARY TABLE\n====================================================================================================\n\nModel                | Precision  | Time (ms)    | Throughput      | Size (MB)   \n----------------------------------------------------------------------------------------------------\nConvNeXt-Tiny        | FP32       |     150.73 |        424.61 |     106.13\n                     | Quantized  |    5823.50 |         10.99 |       7.28\n                     | Improvement|      0.03x |       -97.4% |      93.1%↓\n----------------------------------------------------------------------------------------------------\nCustom-ResNet-SE     | FP32       |     196.96 |        324.94 |      42.75\n                     | Quantized  |   20214.20 |          3.17 |      42.08\n                     | Improvement|      0.01x |       -99.0% |       1.6%↓\n----------------------------------------------------------------------------------------------------\nEfficientNet-B0      | FP32       |      43.29 |       1478.26 |      15.46\n                     | Quantized  |    3618.58 |         17.69 |      15.45\n                     | Improvement|      0.01x |       -98.8% |       0.1%↓\n----------------------------------------------------------------------------------------------------\n\nResults saved to stress_test_results.json\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1}]}